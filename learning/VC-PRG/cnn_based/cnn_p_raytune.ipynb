{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRVp-3z8bvG-",
        "outputId": "e669e94f-a1d7-4b46-a270-69b0b7621f32"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emqe9cDFcPl0"
      },
      "outputs": [],
      "source": [
        "# !pip install -U \"ray[default]\"\n",
        "# !pip install -U tensorboardx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1KHPUYMbwmx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader,random_split\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.io import read_image\n",
        "\n",
        "from filelock import FileLock\n",
        "\n",
        "from ray import tune\n",
        "from ray.air import session\n",
        "from ray.air.checkpoint import Checkpoint\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nso-2shwclrg"
      },
      "outputs": [],
      "source": [
        "DATA_FOLDER = \"/content/drive/MyDrive/Thesis/Experiments/VC-PRG-IMG/\"\n",
        "\n",
        "MODEL_NAME = \"CNN\"\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 50\n",
        "LEARNING_RATE = 1e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZ3xrXv8cpfM"
      },
      "outputs": [],
      "source": [
        "class CNNNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=128, kernel_size=3, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "        )\n",
        "\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "        )\n",
        "\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "        )\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.linear = nn.Linear(in_features=28800, out_features=15)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "    \n",
        "    def forward(self, input_data):\n",
        "        x = self.conv1(input_data)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dropout(x)\n",
        "        logits = self.linear(x)\n",
        "        predictions = self.softmax(logits)\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhPQn36Hcnmk"
      },
      "outputs": [],
      "source": [
        "class VehicleDataset(Dataset):\n",
        "    def __init__(self, X, y, transform=None):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        image = read_image(self.X[idx])\n",
        "        image = image[:3, :, :]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = self.y[idx]\n",
        "        return image, label\n",
        "\n",
        "def get_label(filename):\n",
        "    label = os.path.basename(filename).replace(\".png\", \"\").split(\"-\")[-1]\n",
        "    return int(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(train_data_dir=\"./data\", test_data_dir=\"./data\"):\n",
        "    data_transforms = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    with FileLock(os.path.expanduser(\"~/data.lock\")):\n",
        "        train_files = sorted(glob.glob(os.path.join(train_data_dir, \"*.png\")))\n",
        "        train_labels = [get_label(file) for file in train_files]\n",
        "        test_files = sorted(glob.glob(os.path.join(test_data_dir, \"*.png\")))\n",
        "        test_labels = [get_label(file) for file in test_files]\n",
        "\n",
        "        train_dataset = VehicleDataset(train_files, train_labels, transform=data_transforms)\n",
        "        test_dataset = VehicleDataset(test_files, test_labels, transform=data_transforms)\n",
        "    return train_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(config):\n",
        "  model = CNNNetwork()\n",
        "\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "  model.to(device)\n",
        "\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"wd\"])\n",
        "\n",
        "  loaded_checkpoint = session.get_checkpoint()\n",
        "  if loaded_checkpoint:\n",
        "    with loaded_checkpoint.as_directory() as loaded_checkpoint_dir:\n",
        "      model_state, optimizer_state = torch.load(os.path.join(loaded_checkpoint_dir, \"checkpoint.pt\"))\n",
        "      model.load_state_dict(model_state)\n",
        "      optimizer.load_state_dict(optimizer_state)\n",
        "  \n",
        "  train_data_dir = os.path.abspath(\"/content/drive/MyDrive/Thesis/Experiments//VC-PRG-1_5/\")\n",
        "  test_data_dir = os.path.abspath(\"/content/drive/MyDrive/Thesis/Experiments/VC-PRG-6/\")\n",
        "  train_dataset, test_dataset = load_data(train_data_dir, test_data_dir)\n",
        "\n",
        "  test_abs = int(len(train_dataset) * 0.1)\n",
        "  train_subset, val_subset = random_split(train_dataset, [len(train_dataset) - test_abs, test_abs])\n",
        "\n",
        "  train_dataloader = DataLoader(train_subset, batch_size=int(config[\"batch_size\"]), shuffle=True)\n",
        "  val_dataloader = DataLoader(val_subset, batch_size=int(config[\"batch_size\"]), shuffle=True)\n",
        "\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "    # Training\n",
        "    train_running_loss = 0.0\n",
        "\n",
        "    for idx, (X_train, y_train_trues) in enumerate(train_dataloader, 0):\n",
        "      X_train, y_train_trues = X_train.to(device), y_train_trues.to(device)\n",
        "      \n",
        "      # Zero the gradients paramter\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Forward\n",
        "      y_train_preds = model(X_train)\n",
        "      train_loss = loss_fn(y_train_preds, y_train_trues)\n",
        "      # Backward\n",
        "      train_loss.backward()\n",
        "      # Optimize\n",
        "      optimizer.step()\n",
        "\n",
        "      train_running_loss += train_loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\\t|\\tTrain Loss: {train_running_loss/len(train_dataloader):.5f}\\t|\")\n",
        "\n",
        "    # Validation\n",
        "    val_running_loss = 0.0\n",
        "    val_steps = 0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    for idx, (X_val, y_val_trues) in enumerate(val_dataloader, 0):\n",
        "      with torch.no_grad():\n",
        "        X_val, y_val_trues = X_val.to(device), y_val_trues.to(device)\n",
        "\n",
        "        y_val_preds = model(X_val)\n",
        "        _, predicted = torch.max(y_val_preds.data, 1)\n",
        "        total += y_val_trues.size(0)\n",
        "        correct += (predicted == y_val_trues).sum().item()\n",
        "\n",
        "        val_loss = loss_fn(y_val_preds, y_val_trues)\n",
        "        val_running_loss += val_loss.item()\n",
        "        val_steps += 1\n",
        "\n",
        "    path = f\"/content/drive/MyDrive/Thesis/Experiments/{MODEL_NAME}\"\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    torch.save((model.state_dict(), optimizer.state_dict()), path +\"/checkpoint.pt\")\n",
        "    checkpoint = Checkpoint.from_directory(path)\n",
        "\n",
        "    session.report({\"loss\": (val_running_loss / val_steps), \"accuracy\": correct/total}, checkpoint=checkpoint)\n",
        "\n",
        "  print(\"Finished Traiing\")  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_best_model(best_result):\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "  best_trained_model = CNNNetwork()\n",
        "  best_trained_model.to(device)\n",
        "\n",
        "  checkpoint_path = os.path.join(best_result.checkpoint.to_directory(), \"checkpoint.pt\")\n",
        "\n",
        "  model_state, optimizer_state = torch.load(checkpoint_path)\n",
        "  best_trained_model.load_state_dict(model_state)\n",
        "\n",
        "  train_data_dir = os.path.abspath(\"/content/drive/MyDrive/Thesis/Experiments//VC-PRG-1_5/\")\n",
        "  test_data_dir = os.path.abspath(\"/content/drive/MyDrive/Thesis/Experiments/VC-PRG-6/\")\n",
        "  train_dataset, test_dataset = load_data(train_data_dir, test_data_dir)\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for X_test, y_test_trues in test_dataloader:\n",
        "      X_test, y_test_trues = X_test.to(device), y_test_trues.to(device)\n",
        "      y_test_preds = best_trained_model(X_test)\n",
        "      _, predicted = torch.max(y_test_preds.data, 1)\n",
        "      total += y_test_trues.size(0)\n",
        "      correct += (predicted == y_test_trues).sum().item()\n",
        "  print(\"Best trial test set accuracy: {}\".format(correct / total))   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main(num_samples=10, gpus_per_trial=1):\n",
        "  config = {\n",
        "    \"lr\": tune.loguniform(1e-5, 1e-3),\n",
        "    \"batch_size\": tune.grid_search([32, 64, 128]),\n",
        "    \"wd\": tune.grid_search([0, 0.01, 0.05, 0.025]),\n",
        "  }\n",
        "\n",
        "  scheduler = ASHAScheduler(\n",
        "    max_t=NUM_EPOCHS,\n",
        "    grace_period=1,\n",
        "    reduction_factor=2,\n",
        "  )\n",
        "\n",
        "  tuner = tune.Tuner(\n",
        "      tune.with_resources(\n",
        "          tune.with_parameters(train),\n",
        "          resources={\"cpu\":2, \"gpu\": gpus_per_trial}\n",
        "      ),\n",
        "      tune_config = tune.TuneConfig(\n",
        "          metric=\"loss\",\n",
        "          mode=\"min\",\n",
        "          scheduler=scheduler,\n",
        "          num_samples=num_samples,\n",
        "      ),\n",
        "      param_space=config,\n",
        "  )\n",
        "\n",
        "  results = tuner.fit()\n",
        "\n",
        "  best_result = results.get_best_result(\"loss\", \"min\")\n",
        "\n",
        "  print(\"Best trial config: {}\".format(best_result.config))\n",
        "  print(\"Best trial final validation loss: {}\".format(best_result.metrics[\"loss\"]))\n",
        "  print(\"Best trial final validation accuracy: {}\".format(best_result.metrics[\"accuracy\"]))\n",
        "\n",
        "  test_best_model(best_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "main(num_samples=2, gpus_per_trial=1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "PT",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "43926946470bb827d305256f979d20d06136168b26887207f85f0d5c88e9b0a7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
