{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U \"ray[default]\"\n",
    "# !pip install -U tensorboardx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models import vgg11, VGG11_Weights\n",
    "\n",
    "from filelock import FileLock\n",
    "\n",
    "from ray import tune\n",
    "from ray.air import session\n",
    "from ray.air.checkpoint import Checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"/content/drive/MyDrive/Thesis/Experiments/IDMT-Traffic/\"\n",
    "\n",
    "MODEL_NAME = \"VGG11\"\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IDMTTrafficDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = read_image(self.X[idx])\n",
    "        image = image[:3, :, :]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.y[idx]\n",
    "        return image, label\n",
    "\n",
    "def get_label(filename):\n",
    "    if \"B\" in filename:\n",
    "        return 0\n",
    "    elif \"C\" in filename:\n",
    "        return 1\n",
    "    elif \"M\" in filename:\n",
    "        return 2\n",
    "    elif \"T\" in filename:\n",
    "        return 3        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir=\"./data\"):\n",
    "  data_transforms = transforms.Compose([\n",
    "      transforms.ToPILImage(),\n",
    "      transforms.Resize((224, 224)),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "  ])\n",
    "\n",
    "  with FileLock(os.path.expanduser(\"~/.data.lock\")):\n",
    "    train_files = sorted(glob.glob(os.path.join(data_dir + \"/train/\", \"*.png\")))\n",
    "    train_labels = [get_label(file) for file in train_files]\n",
    "    train_df = pd.DataFrame({\"filename\":train_files, \"label\": train_labels})\n",
    "\n",
    "    test_files = sorted(glob.glob(os.path.join(data_dir + \"/test/\", \"*.png\")))\n",
    "    test_labels = [get_label(file) for file in test_files]\n",
    "    test_df = pd.DataFrame({\"filename\":test_files, \"label\": test_labels})\n",
    "\n",
    "    train_X, train_y = train_df['filename'], train_df['label']\n",
    "    test_X, test_y = test_df['filename'], test_df['label']\n",
    "\n",
    "    train_dataset = VehicleDataset(train_X, train_y, transform=data_transforms)\n",
    "    test_dataset = VehicleDataset(test_X, test_y, transform=data_transforms)\n",
    "\n",
    "  return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config):\n",
    "  model = vgg11(weights=VGG11_Weights.IMAGENET1K_V1)\n",
    "  num_ftrs = model.classifier[6].in_features\n",
    "  features = list(model.classifier.children())[:-1]\n",
    "  features.extend([nn.Linear(num_ftrs, 4)])\n",
    "  model.classifier = nn.Sequential(*features)\n",
    "\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "  model.to(device)\n",
    "\n",
    "  loss_fn = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"wd\"])\n",
    "\n",
    "  loaded_checkpoint = session.get_checkpoint()\n",
    "  if loaded_checkpoint:\n",
    "    with loaded_checkpoint.as_directory() as loaded_checkpoint_dir:\n",
    "      model_state, optimizer_state = torch.load(os.path.join(loaded_checkpoint_dir, \"checkpoint.pt\"))\n",
    "      model.load_state_dict(model_state)\n",
    "      optimizer.load_state_dict(optimizer_state)\n",
    "  \n",
    "  data_dir = os.path.abspath(\"/content/drive/MyDrive/Thesis/Experiments/VC-PRG-IMG/\")\n",
    "  train_dataset, test_dataset = load_data(data_dir)\n",
    "\n",
    "  test_abs = int(len(train_dataset) * 0.1)\n",
    "  train_subset, val_subset = random_split(train_dataset, [test_abs, len(train_dataset) - test_abs])\n",
    "\n",
    "  train_dataloader = DataLoader(train_subset, batch_size=int(config[\"batch_size\"]), shuffle=True)\n",
    "  val_dataloader = DataLoader(val_subset, batch_size=int(config[\"batch_size\"]), shuffle=True)\n",
    "\n",
    "  for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    # Training\n",
    "    train_running_loss = 0.0\n",
    "\n",
    "    for idx, (X_train, y_train_trues) in enumerate(train_dataloader, 0):\n",
    "      X_train, y_train_trues = X_train.to(device), y_train_trues.to(device)\n",
    "      \n",
    "      # Zero the gradients paramter\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Forward\n",
    "      y_train_preds = model(X_train)\n",
    "      train_loss = loss_fn(y_train_preds, y_train_trues)\n",
    "      # Backward\n",
    "      train_loss.backward()\n",
    "      # Optimize\n",
    "      optimizer.step()\n",
    "\n",
    "      train_running_loss += train_loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\\t|\\tTrain Loss: {train_running_loss/len(train_dataloader):.5f}\\t|\")\n",
    "\n",
    "    # Validation\n",
    "    val_running_loss = 0.0\n",
    "    val_steps = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for idx, (X_val, y_val_trues) in enumerate(val_dataloader, 0):\n",
    "      with torch.no_grad():\n",
    "        X_val, y_val_trues = X_val.to(device), y_val_trues.to(device)\n",
    "\n",
    "        y_val_preds = model(X_val)\n",
    "        _, predicted = torch.max(y_val_preds.data, 1)\n",
    "        total += y_val_trues.size(0)\n",
    "        correct += (predicted == y_val_trues).sum().item()\n",
    "\n",
    "        val_loss = loss_fn(y_val_preds, y_val_trues)\n",
    "        val_running_loss += val_loss.item()\n",
    "        val_steps += 1\n",
    "\n",
    "    path = f\"/content/drive/MyDrive/Thesis/Experiments/{MODEL_NAME}\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    torch.save((model.state_dict(), optimizer.state_dict()), path +\"/checkpoint.pt\")\n",
    "    checkpoint = Checkpoint.from_directory(path)\n",
    "\n",
    "    session.report({\"loss\": (val_running_loss / val_steps), \"accuracy\": correct/total}, checkpoint=checkpoint)\n",
    "\n",
    "  print(\"Finished Traiing\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_best_model(best_result):\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "  best_trained_model = vgg11(weights=VGG11_Weights.IMAGENET1K_V1)\n",
    "  num_ftrs = best_trained_model.classifier[6].in_features\n",
    "  features = list(best_trained_model.classifier.children())[:-1]\n",
    "  features.extend([nn.Linear(num_ftrs, 4)])\n",
    "  best_trained_model.classifier = nn.Sequential(*features)\n",
    "\n",
    "  checkpoint_path = os.path.join(best_result.checkpoint.to_directory(), \"checkpoint.pt\")\n",
    "\n",
    "  model_state, optimizer_state = torch.load(checkpoint_path)\n",
    "  best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "  data_dir = os.path.abspath(\"/content/drive/MyDrive/Thesis/Experiments/IDMT_Traffic_IMG/\")\n",
    "  train_dataset, test_dataset = load_data(data_dir)\n",
    "  test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  with torch.no_grad():\n",
    "    for X_test, y_test_trues in test_dataloader:\n",
    "      X_test, y_test_trues = X_test.to(device), y_test_trues.to(device)\n",
    "      y_test_preds = best_trained_model(X_test)\n",
    "      _, predicted = torch.max(y_test_preds.data, 1)\n",
    "      total += y_test_trues.size(0)\n",
    "      correct += (predicted == y_test_trues).sum().item()\n",
    "  print(\"Best trial test set accuracy: {}\".format(correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_samples=10, gpus_per_trial=1):\n",
    "  config = {\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-3),\n",
    "    \"batch_size\": tune.grid_search([32, 64, 128]),\n",
    "    \"wd\": tune.grid_search([0, 0.01, 0.05, 0.025]),\n",
    "  }\n",
    "\n",
    "  scheduler = ASHAScheduler(\n",
    "    max_t=NUM_EPOCHS,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2,\n",
    "  )\n",
    "\n",
    "  tuner = tune.Tuner(\n",
    "      tune.with_resources(\n",
    "          tune.with_parameters(train),\n",
    "          resources={\"cpu\":2, \"gpu\": gpus_per_trial}\n",
    "      ),\n",
    "      tune_config = tune.TuneConfig(\n",
    "          metric=\"loss\",\n",
    "          mode=\"min\",\n",
    "          scheduler=scheduler,\n",
    "          num_samples=num_samples,\n",
    "      ),\n",
    "      param_space=config,\n",
    "  )\n",
    "\n",
    "  results = tuner.fit()\n",
    "\n",
    "  best_result = results.get_best_result(\"loss\", \"min\")\n",
    "\n",
    "  print(\"Best trial config: {}\".format(best_result.config))\n",
    "  print(\"Best trial final validation loss: {}\".format(best_result.metrics[\"loss\"]))\n",
    "  print(\"Best trial final validation accuracy: {}\".format(best_result.metrics[\"accuracy\"]))\n",
    "\n",
    "  test_best_model(best_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(num_samples=2, gpus_per_trial=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43926946470bb827d305256f979d20d06136168b26887207f85f0d5c88e9b0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
