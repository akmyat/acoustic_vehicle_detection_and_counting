{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install torchaudio\n",
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchaudio\n",
    "from torchaudio.transforms import Resample\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import wget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download and Unzip VC-PRG-1_5.zip\n",
    "# !wget http://cmp.felk.cvut.cz/data/audio_vc/audio/VC-PRG-1_5.zip\n",
    "# !unzip VC-PRG-1_5.zip\n",
    "# !rm VC-PRG-1_5.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download and Unzip VC-PRG-6.zip\n",
    "# !wget http://cmp.felk.cvut.cz/data/audio_vc/audio/VC-PRG-6.zip\n",
    "# !unzip VC-PRG-6.zip\n",
    "# !rm VC-PRG-6.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"/home/penguin/Data/thesis/learning/vehicle_counting/VC-PRG-1_5/\"\n",
    "TARGET_SAMPLE_RATE = 44100 # Hz\n",
    "SAMPLE_LENGTH =  20 # seconds\n",
    "BATCH_SIZE = 10\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VehicleCountingDataset(Dataset):\n",
    "    def __init__(self, dataset_path, target_sample_rate, signal_len, transformation, device):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.signal_len = signal_len\n",
    "        self.num_samples = int(self.signal_len * self.target_sample_rate)\n",
    "        self.transformation = transformation\n",
    "        self.device = device\n",
    "\n",
    "        self.audio_files = glob.glob(os.path.join(self.dataset_path, \"*.wav\"))\n",
    "        self.vc_files = glob.glob(os.path.join(self.dataset_path, \"*.txt\"))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        label = self._get_vc(item)\n",
    "        signal, sample_rate = self._get_signal(item)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sample_rate)      \n",
    "        signal = self._mix_down_if_necessary(signal)    \n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        mel_spec = self.transformation(signal)\n",
    "        return mel_spec, label\n",
    "\n",
    "    def _get_signal(self, item):\n",
    "        signal, sample_rate = torchaudio.load(self.audio_files[item])\n",
    "        return signal, sample_rate\n",
    "\n",
    "    def _get_vc(self, item):\n",
    "        vc = 0\n",
    "        with open(self.vc_files[item], 'r') as f:\n",
    "            vc = len(f.readlines())\n",
    "        return vc\n",
    "    \n",
    "    def _resample_if_necessary(self, signal, sample_rate):\n",
    "        if sample_rate != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, self.target_sample_rate).to(self.device)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "    \n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "\n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "    \n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        if signal.shape[1] < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - signal.shape[1]\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "def create_data_loader(data, batch_size):\n",
    "    dataloader = DataLoader(data, batch_size=batch_size)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.linear = nn.Linear(in_features=22400, out_features=10)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        x = self.conv1(input_data)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear(x)\n",
    "        predictions = self.softmax(logits)\n",
    "        return predictions\n",
    "\n",
    "def train_single_epoch(model, data_loader, loss_fn, optimizer, device):\n",
    "    for feature, target in data_loader:\n",
    "        feature, target = feature.to(device), target.to(device)\n",
    "\n",
    "        # Loss\n",
    "        predictions = model(feature)\n",
    "        loss = loss_fn(predictions, target)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "    return loss.item()\n",
    "\n",
    "def train(model, data_loader, loss_fn, optimizer, epochs, device):\n",
    "    loss_history = {}\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch: {epoch +1}\")\n",
    "        loss = train_single_epoch(model, data_loader, loss_fn, optimizer, device)\n",
    "        loss_history[epoch] = loss\n",
    "        print(\"----------------------------------------------------------------------------------------\")\n",
    "    print(\"Training finished\")\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Feature Extractor\n",
    "mel_spectrogram_transformer = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=TARGET_SAMPLE_RATE,\n",
    "    n_fft=4096,\n",
    "    hop_length=1638,\n",
    "    n_mels=64\n",
    ").to(device)\n",
    "\n",
    "# Dataset\n",
    "vcd = VehicleCountingDataset(DATASET_PATH, TARGET_SAMPLE_RATE, SAMPLE_LENGTH, mel_spectrogram_transformer, device)\n",
    "\n",
    "# Data loader\n",
    "train_dataloader = create_data_loader(vcd, BATCH_SIZE)\n",
    "\n",
    "# Model\n",
    "model = CNNNetwork().to(device)\n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Train\n",
    "loss_history = train(model, train_dataloader, loss_fn, optimizer, EPOCHS, device)\n",
    "torch.save(model.state_dict(), \"vcd_cnn_model.pt\")\n",
    "plt.plot(loss_history.values())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43926946470bb827d305256f979d20d06136168b26887207f85f0d5c88e9b0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
