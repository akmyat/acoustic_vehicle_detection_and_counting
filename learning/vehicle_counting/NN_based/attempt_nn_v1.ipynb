{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 18:11:32.924050: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-19 18:11:56.280104: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.7/lib64\n",
      "2023-01-19 18:11:56.280777: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.7/lib64\n",
      "2023-01-19 18:11:56.280805: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras.backend as K\n",
    "from keras import regularizers\n",
    "\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs_NN = 100\n",
    "loss = 'MSE'\n",
    "\n",
    "# dataset_id = 'adj5_featLMS48_f[1000-22050]'\n",
    "# dataset_train = f'datasets/dataset_triangle_len540_thr0.75_{dataset_id}.h5'\n",
    "# dataset_test = f'datasets/dataset_triangle_len540_thr0.75_{dataset_id}_unseen.h5'\n",
    "\n",
    "extension = 15\n",
    "layer_units_NN1 = [64, 64, 1]\n",
    "layer_units_NN2 = [2 * extension+1, extension, 1]\n",
    "\n",
    "reg_coeff_NN1 = 1e-4\n",
    "reg_coeff_NN2 = 5e-6\n",
    "\n",
    "runs = 1\n",
    "\n",
    "rm_filt_len = [5, 3]                # Running mean filter length (filtering prior to peak detection)\n",
    "peak_det_thr = [40, 20]     # A vehicle is detected if detected peak of the inverted predicted distance has magnitude larger than 40% Td or prominence larger than 20%Td\n",
    "\n",
    "time_samples_num = 540\n",
    "tdt = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_MAE(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true))\n",
    "\n",
    "\n",
    "def get_nn_model(input_dim, layer_units, loss, reg_coeff):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layer_units[0], input_dim=input_dim, activation='relu', kernel_regularizer=regularizers.l2(reg_coeff)))\n",
    "    model.add(BatchNormalization())\n",
    "    for lu in layer_units[1:-1]:\n",
    "        model.add(Dense(lu, activation='relu', kernel_regularizer=regularizers.l2(reg_coeff)))\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dense(layer_units[-1], activation='linear'))\n",
    "\n",
    "    if loss == 'MAE':\n",
    "        model.compile(optimizer='Adam', loss=loss_MAE)\n",
    "    elif loss == 'MSE':\n",
    "        model.compile(optimizer='Adam', loss='mse')\n",
    "    # model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def extend_features(feat, points, step=1):\n",
    "    refer_points = 10\n",
    "    (d0, d1) = feat.shape[0], feat.shape[1]\n",
    "    feat_extend = np.zeros((d0, d1, 2 * points + 1))\n",
    "    \n",
    "    for i in range(d0):\n",
    "        feat_frame = np.zeros((d1, 2 * points + 1))\n",
    "        feat_frame[:, points] = feat[i,:].copy()\n",
    "        feat_curr = None\n",
    "        feat_copy = feat[i,:].copy()\n",
    "        for k in range(points):\n",
    "            feat_temp = feat_copy if k == 0 else feat_curr\n",
    "            feat_curr = np.zeros_like(feat_temp)\n",
    "            arr_ext = running_mean(feat_temp[:refer_points], 3)\n",
    "            add_left, coef, interc = linear_interp(arr_ext, step, 'L')\n",
    "            feat_curr[:step] = add_left\n",
    "            feat_curr[step:] = feat_temp[:-step].copy()\n",
    "            feat_frame[:, points-1-k] = feat_curr   \n",
    "        for k in range(points):\n",
    "            feat_temp = feat_copy if k == 0 else feat_curr\n",
    "            feat_curr = np.zeros_like(feat_temp)\n",
    "            arr_ext = running_mean(feat_temp[-refer_points:], 3)\n",
    "            add_right, coef, interc = linear_interp(arr_ext, step, 'R')\n",
    "            feat_curr[:-step] = feat_temp[step:].copy()\n",
    "            feat_curr[-step:] = add_right\n",
    "            feat_frame[:, points+1+k] = feat_curr\n",
    "        feat_extend[i,:,:] = feat_frame\n",
    "\n",
    "    return feat_extend\n",
    "\n",
    "\n",
    "def linear_interp(array, point_no, direction='L'):\n",
    "    assert direction == 'L' or direction == 'R', \"Extrapolation direction not valid\"\n",
    "    \n",
    "    arr_len = np.size(array)\n",
    "    x = np.linspace(0,arr_len-1,arr_len).reshape(-1,1)\n",
    "    lin_reg = LinearRegression().fit(x, array.reshape(-1,1))\n",
    "    coef = np.float(lin_reg.coef_)\n",
    "    interc = np.float(lin_reg.intercept_)\n",
    "    if direction == 'L':\n",
    "        x_extra = np.linspace(-point_no, -1, point_no)\n",
    "    else:\n",
    "        x_extra = np.linspace(arr_len, arr_len + point_no - 1, point_no)\n",
    "    \n",
    "    y_extra = coef * x_extra + interc\n",
    "    return y_extra, coef, interc\n",
    "\n",
    "def running_mean(x, n):\n",
    "    if n % 2 == 0:\n",
    "        raise ValueError(\"Filter length is not odd\")\n",
    "\n",
    "    aver = np.convolve(x, np.ones((n,)) / n, mode='same')\n",
    "    n_half = int((n - 1) / 2)\n",
    "    corr_length = np.array(range(n_half + 1, n)) / n\n",
    "    aver[:n_half] = np.divide(aver[:n_half], corr_length)\n",
    "    aver[-n_half:] = np.divide(aver[-n_half:], corr_length[::-1])\n",
    "\n",
    "    return aver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def car_detection(y_pred, y_gt, time_dist_thr, dec_thr, post_proc=True, smart_PP=False, filt_len=[7,5,3], peak_detect_thresh=[0,0,0.01]):\n",
    "    # assert y_pred.shape == y_gt.shape, \"Predicted and ground truth arrays are not of same dimensions\"\n",
    "\n",
    "    if post_proc:\n",
    "        y_pred = detection_postprocessing(y_pred, filt_len)\n",
    "    correct, false_pos, false_neg = \\\n",
    "        compare_prediction_and_gt(y_pred, y_gt, time_dist_thr, dec_thr, smart_PP, peak_detect_thresh)\n",
    "\n",
    "    return correct, false_pos, false_neg\n",
    "\n",
    "def compare_prediction_and_gt(y_pred, y_gt, time_dist_thresh, dec_thresh, smart_peak_det, peak_det_thresh):\n",
    "    cars_correct = false_pos = false_neg = 0\n",
    "    for k in range(y_pred.shape[0]):\n",
    "        gt_row = y_gt[k, :].flatten()\n",
    "        peaks_gt = find_peaks(np.max(gt_row) - gt_row)[0]\n",
    "        gt_intervals = np.zeros((peaks_gt.size, 2))\n",
    "        for j in range(peaks_gt.size):\n",
    "            aux = peaks_gt[j]\n",
    "            while aux > 0 and gt_row[aux] < gt_row[aux - 1]:\n",
    "                aux -= 1\n",
    "            gt_intervals[j, 0] = aux\n",
    "            aux = peaks_gt[j]\n",
    "            while aux < gt_row.size - 1 and gt_row[aux] < gt_row[aux + 1]:\n",
    "                aux += 1\n",
    "            gt_intervals[j, 1] = aux\n",
    "\n",
    "        pred_row = y_pred[k, :].flatten()\n",
    "        peak_prom = 0.01 if smart_peak_det else 0.05\n",
    "        peaks_pos, peaks_stat = smart_peak_picking(np.max(pred_row) - pred_row,\n",
    "                                                   peak_prom=peak_prom,\n",
    "                                                   thresholds=peak_det_thresh,\n",
    "                                                   peak_filtering=smart_peak_det)\n",
    "        \n",
    "        pred_peak_pos = peaks_pos[np.argwhere(pred_row[peaks_pos] < dec_thresh)].flatten()\n",
    "        for j in range(pred_peak_pos.size):\n",
    "            within = np.logical_and(gt_intervals[:, 0] < pred_peak_pos[j], \n",
    "                                    gt_intervals[:, 1] > pred_peak_pos[j])\n",
    "            ind_within = np.argwhere(within).flatten()\n",
    "            if ind_within.size > 0:\n",
    "                cars_correct += 1\n",
    "                gt_intervals = np.delete(gt_intervals, ind_within[0], axis=0)\n",
    "            else:\n",
    "                false_pos += 1\n",
    "        false_neg += gt_intervals.shape[0]\n",
    "\n",
    "    return cars_correct, false_pos, false_neg\n",
    "\n",
    "def smart_peak_picking(sig, peak_prom, thresholds, peak_filtering):\n",
    "    if not peak_filtering:\n",
    "        peaks_det = find_peaks(sig, prominence=peak_prom)\n",
    "        peaks_pos, peaks_stat = peaks_det[0], peaks_det[1]\n",
    "        return peaks_pos, peaks_stat\n",
    "    \n",
    "    peaks_det = find_peaks(sig, prominence=peak_prom)\n",
    "    peaks_pos, peaks_stat = peaks_det[0], peaks_det[1]\n",
    "    if peaks_pos.size > 0:\n",
    "        keep_peaks_ind = np.array([])\n",
    "        for k in range(peaks_pos.size):\n",
    "            if sig[peaks_pos[k]] > thresholds[0] or peaks_stat['prominences'][k] > thresholds[1]:\n",
    "                keep_peaks_ind = np.append(keep_peaks_ind, [k])\n",
    "        keep_peaks_ind = keep_peaks_ind.astype(np.int64)\n",
    "        peaks_pos = peaks_pos[keep_peaks_ind]\n",
    "        peaks_stat['prominences'] = peaks_stat['prominences'][keep_peaks_ind]\n",
    "        peaks_stat['left_bases'] = peaks_stat['left_bases'][keep_peaks_ind]\n",
    "        peaks_stat['right_bases'] = peaks_stat['right_bases'][keep_peaks_ind]\n",
    "\n",
    "    return peaks_pos, peaks_stat\n",
    "\n",
    "def detection_postprocessing(y_pred, filt_len):\n",
    "    y_pred_proc = y_pred.copy()\n",
    "    for k in range(y_pred_proc.shape[0]):\n",
    "        row = y_pred_proc[k, :].flatten()\n",
    "        for fl in filt_len:\n",
    "            row = running_mean(row, fl)\n",
    "        y_pred_proc[k, :] = row\n",
    "\n",
    "    return y_pred_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = \"datasets/train_dataset.h5\"\n",
    "dataset_test = \"datasets/test_dataset.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 18:19:21.888952: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-19 18:19:24.388896: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.7/lib64\n",
      "2023-01-19 18:19:24.568855: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-01-19 18:19:24.570941: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-19 18:19:27.710857: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 228096000 exceeds 10% of free system memory.\n",
      "2023-01-19 18:22:18.221682: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 285120000 exceeds 10% of free system memory.\n",
      "2023-01-19 18:22:20.932958: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 196162560 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss NN2: 0.00153,  run: 0\n"
     ]
    }
   ],
   "source": [
    "for run_No in range(runs):\n",
    "    hf = h5py.File(dataset_train, 'r')\n",
    "    x_train = np.array(hf['features'], dtype=np.float64)\n",
    "    y_train = np.array(hf['labels'], dtype=np.float64)\n",
    "    vc_train = np.array(hf['vehicle_counts']).astype('int')\n",
    "    hf.close()\n",
    "\n",
    "    hf = h5py.File(dataset_test, 'r')\n",
    "    x_test = np.array(hf['features'], dtype=np.float64)\n",
    "    y_test = np.array(hf['labels'], dtype=np.float64)\n",
    "    vc_test = np.array(hf['vehicle_counts']).astype('int')\n",
    "    hf.close()\n",
    "    \n",
    "    if run_No == 0:\n",
    "        test_reg = np.zeros((runs, vc_test.size, time_samples_num))\n",
    "\n",
    "    x_train = x_train.reshape(x_train.shape[0] * x_train.shape[1], x_train.shape[2])\n",
    "    y_train = y_train.reshape(-1, )\n",
    "    x_test = x_test.reshape(x_test.shape[0] * x_test.shape[1], x_test.shape[2])\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    # Training NN model\n",
    "    K.clear_session()  # for multiple runs, clear the session\n",
    "\n",
    "    # First NN\n",
    "    model_NN_1 = get_nn_model(x_train.shape[-1], layer_units_NN1, loss, reg_coeff_NN1)\n",
    "    callbacks = [ModelCheckpoint(filepath='models/NN1_unseen.hdf5', monitor='val_loss', verbose=0, mode='min', save_best_only=True)]\n",
    "    \n",
    "    #  Shuffle input data before training  \n",
    "    rand_perm = np.random.RandomState().permutation(x_train.shape[0])\n",
    "    x_train_nn = x_train[rand_perm, :].copy()\n",
    "    y_train_nn = y_train[rand_perm].copy()\n",
    "\n",
    "    history = model_NN_1.fit(x_train_nn, \n",
    "                             y_train_nn,\n",
    "                             validation_split=0.2,\n",
    "                             epochs=epochs_NN,\n",
    "                             verbose=0, \n",
    "                             batch_size=batch_size, \n",
    "                             callbacks=callbacks, \n",
    "                             shuffle=True)\n",
    "    loss_NN1 = history.history['val_loss'][-1]\n",
    "    # print(f\"Loss NN1: {loss_NN1:.5f}\")\n",
    "    model_NN_1.load_weights('models/NN1.hdf5')\n",
    "    # Prediction (training and test data)\n",
    "    y_train_pred_NN = model_NN_1.predict(x_train, verbose=0, batch_size=batch_size)\n",
    "    y_train_pred_NN = y_train_pred_NN.reshape(-1, time_samples_num)\n",
    "    y_test_pred_NN = model_NN_1.predict(x_test, verbose=0, batch_size=batch_size)\n",
    "    y_test_pred_NN = y_test_pred_NN.reshape(-1, time_samples_num)\n",
    "    \n",
    "    # Second NN\n",
    "    x_train_2 = extend_features(y_train_pred_NN, extension)\n",
    "    x_test_2 = extend_features(y_test_pred_NN, extension)\n",
    "    \n",
    "    x_train_2 = x_train_2.reshape(x_train_2.shape[0] * x_train_2.shape[1], x_train_2.shape[2])\n",
    "    x_test_2 = x_test_2.reshape(x_test_2.shape[0] * x_test_2.shape[1], x_test_2.shape[2])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train_2)\n",
    "    x_train_2 = scaler.transform(x_train_2)\n",
    "    x_test_2 = scaler.transform(x_test_2)\n",
    "    \n",
    "    model_NN_2 = get_nn_model(x_train_2.shape[-1], layer_units_NN2, loss, reg_coeff_NN2)\n",
    "    callbacks = [ModelCheckpoint(filepath='models/NN2.hdf5', monitor='val_loss', verbose=0, mode='min', save_best_only=True)]\n",
    "    #  Shuffle input data before training\n",
    "    rand_perm = np.random.RandomState().permutation(x_train_2.shape[0])\n",
    "    x_train_nn2 = x_train_2[rand_perm, :].copy()\n",
    "    y_train_nn2 = y_train[rand_perm].copy()\n",
    "    \n",
    "    history = model_NN_2.fit(x_train_nn2, \n",
    "                             y_train_nn2, \n",
    "                             validation_split=0.2,\n",
    "                             epochs=epochs_NN, \n",
    "                             verbose=0, \n",
    "                             batch_size=batch_size, \n",
    "                             callbacks=callbacks, \n",
    "                             shuffle=True)\n",
    "    loss_NN2 = history.history['val_loss'][-1]\n",
    "    print(f\"Loss NN2: {loss_NN2:.5f},  run: {run_No}\")\n",
    "    model_NN_2.load_weights('models/NN2_unseen.hdf5')\n",
    "    \n",
    "    y_test_pred_NN_NN = model_NN_2.predict(x_test_2, verbose=0, batch_size=batch_size)\n",
    "    y_test_pred_NN_NN = y_test_pred_NN_NN.reshape(-1, time_samples_num)\n",
    "    test_reg[run_No,:,:] = y_test_pred_NN_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.reshape(test_reg[0, :, :].shape)\n",
    "thresh_perc = np.arange(0, 101, 1)\n",
    "thr_len = thresh_perc.size\n",
    "correct, false_pos, false_neg = np.zeros((runs, thr_len)), np.zeros((runs, thr_len)), np.zeros((runs, thr_len))\n",
    "RVCE = np.zeros((runs, thr_len))\n",
    "\n",
    "for run_No in range(runs):\n",
    "    test_pred = test_reg[run_No,:,:]\n",
    "    \n",
    "    # Calculation of probs and RVCE\n",
    "    total_VC = np.sum(vc_test)\n",
    "    for j in range(thr_len):\n",
    "        (corr, fp, fn) = car_detection(test_pred,\n",
    "                                          y_test,\n",
    "                                          tdt,\n",
    "                                          (thresh_perc[j] / 100) * tdt,\n",
    "                                          post_proc=True,\n",
    "                                          smart_PP=True,\n",
    "                                          filt_len=rm_filt_len,\n",
    "                                          peak_detect_thresh=[x*tdt/100 for x in peak_det_thr])\n",
    "        correct[run_No,j], false_pos[run_No,j], false_neg[run_No,j] = \\\n",
    "                round(corr/total_VC*100, 2), round(fp/total_VC*100, 2), round(fn/total_VC*100, 2)\n",
    "        RVCE[run_No,j] = (total_VC - (corr + fp)) / total_VC * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[99.65517241, 99.65517241, 99.48275862, 99.31034483, 99.13793103,\n",
       "        98.10344828, 94.82758621, 87.5862069 , 74.31034483, 61.72413793,\n",
       "        46.72413793, 34.31034483, 26.55172414, 20.68965517, 16.89655172,\n",
       "        14.48275862, 12.4137931 , 10.86206897,  8.96551724,  8.62068966,\n",
       "         8.27586207,  7.75862069,  7.24137931,  7.24137931,  6.89655172,\n",
       "         6.72413793,  6.55172414,  6.37931034,  6.20689655,  6.03448276,\n",
       "         5.68965517,  5.68965517,  5.68965517,  5.51724138,  5.34482759,\n",
       "         5.        ,  4.48275862,  3.96551724,  3.79310345,  3.79310345,\n",
       "         3.62068966,  3.44827586,  3.10344828,  2.93103448,  2.75862069,\n",
       "         2.5862069 ,  2.06896552,  2.06896552,  2.06896552,  1.55172414,\n",
       "         1.03448276,  0.86206897,  0.86206897,  0.86206897,  0.51724138,\n",
       "         0.51724138,  0.51724138,  0.17241379,  0.17241379,  0.        ,\n",
       "         0.        , -0.17241379, -0.34482759, -0.68965517, -0.86206897,\n",
       "        -0.86206897, -0.86206897, -1.03448276, -1.20689655, -1.20689655,\n",
       "        -1.37931034, -1.37931034, -1.37931034, -1.55172414, -1.72413793,\n",
       "        -1.72413793, -1.72413793, -1.89655172, -1.89655172, -1.89655172,\n",
       "        -1.89655172, -1.89655172, -1.89655172, -1.89655172, -1.89655172,\n",
       "        -1.89655172, -1.89655172, -1.89655172, -1.89655172, -1.89655172,\n",
       "        -1.89655172, -1.89655172, -1.89655172, -1.89655172, -1.89655172,\n",
       "        -1.89655172, -1.89655172, -1.89655172, -1.89655172, -1.89655172,\n",
       "        -1.89655172]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RVCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c71ccffe01fdfb295996a0f65a7af17abd82c3a990a1d3150e0ac70e6643c9b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
