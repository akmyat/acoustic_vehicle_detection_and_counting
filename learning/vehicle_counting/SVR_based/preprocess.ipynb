{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "from scipy.fftpack import fft\n",
    "from scipy import interpolate\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import librosa\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from importlib import reload\n",
    "import h5py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def load_audio(self, audio_filename, signal_length=None, sample_rate=44100):\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        fs, y = wav.read(audio_filename)\n",
    "\n",
    "        if y.dtype == np.int16:\n",
    "            y = y / 32768.0\n",
    "        if len(y.shape) == 2:\n",
    "            y = y[:, 0]\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        if signal_length is not None:\n",
    "            if y.size / fs > signal_length:\n",
    "                # Cut the signal if it is too long\n",
    "                y = y[:int(signal_length *fs)]\n",
    "            else:\n",
    "                # Pad the signal with zeros if it is too short\n",
    "                y = np.pad(y, int((signal_length * fs - y.size) / 2), mode='constant')\n",
    "        \n",
    "        if fs != sample_rate:\n",
    "            y = librosa.core.resample(y, fs, sample_rate)\n",
    "            fs = sample_rate\n",
    "        \n",
    "        return y, fs\n",
    "\n",
    "    def extract_feature(self, signal, sample_rate, win_len, hop_len, mel_band_num, adj_feat_num, rm_filts_len):\n",
    "        window = np.hamming(win_len)\n",
    "        y = signal\n",
    "        fs = sample_rate\n",
    "\n",
    "        # Pad the signal to take into account window at the edges\n",
    "        y = np.pad(y, int(win_len / 2), mode='constant')\n",
    "        nb_frames = int((y.size - win_len) / hop_len) + 1\n",
    "\n",
    "        fft_len = win_len\n",
    "        fft_mel_bands = librosa.filters.mel(fs, fft_len, mel_band_num, fmin=0.0).T\n",
    "        fft_en = np.zeros((nb_frames, int(1 + fft_len / 2)))\n",
    "\n",
    "        lms = np.zeros((nb_frames, mel_band_num))\n",
    "        st_energy = np.zeros((nb_frames,))\n",
    "        time_vec = np.zeros((nb_frames,))\n",
    "\n",
    "        for i in range(nb_frames):\n",
    "            y_win = y[i * hop_len: i * hop_len + win_len] * window\n",
    "            fft_aux = np.abs(fft(y_win)[: 1 + int(fft_len / 2)])\n",
    "            fft_aux[fft_aux == 0] = np.finfo(np.float32).eps\n",
    "            fft_en[i, :] = fft_aux\n",
    "            lms[i, :] = np.dot(fft_en[i, :] ** 2, fft_mel_bands)\n",
    "            st_energy[i] = np.sum(np.power(y[i * hop_len: i * hop_len + win_len], 2))\n",
    "            time_vec[i] = (i * hop_len) / fs\n",
    "        short_time_ft = fft_en.T\n",
    "\n",
    "        ste_feat = self._energy_feature(st_energy.copy(), adj_feat_num, rm_filts_len)\n",
    "        trf_feat = self._trf_feature(short_time_ft.copy(), adj_feat_num, rm_filts_len)\n",
    "        hfp_feat = self._hfp_feature(short_time_ft.copy(), fs, adj_feat_num, rm_filts_len)\n",
    "        lms_feat = self._lms_feature(lms)\n",
    "        features_full = np.concatenate((ste_feat, trf_feat, hfp_feat, lms_feat), axis=0).T\n",
    "\n",
    "        return features_full, time_vec\n",
    "\n",
    "    def _energy_feature(self, energy, extra_points, rm_filts_len):\n",
    "        energy = self._filter_and_normalize(energy, rm_filts_len)\n",
    "        if extra_points > 0:\n",
    "            energy = self._extend_feature(energy, extra_points)\n",
    "        else:\n",
    "            energy = energy.reshape(1, -1)\n",
    "        return energy\n",
    "    \n",
    "    def _extend_feature(self, feat, extra_points):\n",
    "        refer_points = 10 # Number of edge points used in extrapolation\n",
    "        feat_len = feat.size\n",
    "        add_left = self._extrapolate_points(feat[:refer_points], extra_points, 'L', np.finfo(np.float32).eps)\n",
    "        feat_exp = np.append(add_left, feat)\n",
    "        add_right = self._extrapolate_points(feat[-refer_points:], extra_points, 'R', np.finfo(np.float32).eps)\n",
    "        feat_exp = np.append(feat_exp, add_right)\n",
    "        feat_exp = [feat_exp[i: i + feat_len] for i in range(2 * extra_points + 1)]\n",
    "        feat_exp = np.array(feat_exp)\n",
    "\n",
    "        return feat_exp\n",
    "\n",
    "    def _extrapolate_points(self, x, point_no, direction='R', threshold=None):\n",
    "        assert direction == 'L' or direction == 'R', \"Extrapolation direction not valid\"\n",
    "        extra_points = np.zeros((point_no, ))\n",
    "        refer_points = x.size\n",
    "        interp = interpolate.interp1d(range(refer_points), x, fill_value=\"extrapolate\", kind=\"quadratic\")\n",
    "        for k in range(point_no):\n",
    "            if direction == 'L':\n",
    "                extra_points[k] = interp(-point_no + k)\n",
    "            else:\n",
    "                extra_points[k] = interp(refer_points + k)\n",
    "        \n",
    "        if threshold is not None:\n",
    "            extra_points[extra_points < threshold] = threshold\n",
    "        return extra_points\n",
    "    \n",
    "    def _lms_feature(self, lms):\n",
    "        lms[lms == 0] = np.finfo(np.float32).eps\n",
    "        lms = np.log(lms.T)\n",
    "\n",
    "        mean_diff = np.mean(np.mean(lms[:, 2:5], axis=1) - np.mean(lms[:, 0:2], axis=1))\n",
    "        mean_diff_thresh = 3.0\n",
    "        inter_points = 7\n",
    "        if mean_diff > mean_diff_thresh:\n",
    "            for k in range(lms.shape[0]):\n",
    "                inter_val, coef, interc = self._linear_interp(lms[k, 2:2+inter_points], 2, direction='L')\n",
    "                lms[k, 0:2] = inter_val\n",
    "        return lms\n",
    "    \n",
    "    def _linear_interp(self, array, point_no, direction='L'):\n",
    "        assert direction == 'L' or direction == 'R', \"Extrapolation direction not valid\"\n",
    "\n",
    "        arr_len = np.size(array)\n",
    "        x = np.linspace(0, arr_len-1, arr_len).reshape(-1, 1)\n",
    "        lin_reg = LinearRegression().fit(x, array.reshape(-1, 1))\n",
    "        coef = np.float(lin_reg.coef_)\n",
    "        interc = np.float(lin_reg.intercept_)\n",
    "\n",
    "        if direction == 'L':\n",
    "            x_extra = np.linspace(-point_no, -1, point_no)\n",
    "        else:\n",
    "            x_extra = np.linspace(arr_len, arr_len+point_no-1, point_no)\n",
    "        \n",
    "        y_extra = coef * x_extra + interc\n",
    "        return y_extra, coef, interc\n",
    "\n",
    "    def _trf_feature(self, stft, extra_points, rm_filts_len):\n",
    "        stft = gaussian_filter(stft, sigma=3, truncate=3)\n",
    "        thresh = 3 * np.median(stft)\n",
    "        trf = np.zeros((stft.shape[1], ))\n",
    "        for k in range(stft.shape[1]):\n",
    "            inds = np.argwhere(stft[:, k] > thresh)\n",
    "            trf[k] = inds[-1] if inds.size > 0 else 0\n",
    "        trf = self._filter_and_normalize(trf, rm_filts_len)\n",
    "        if extra_points > 0:\n",
    "            trf = self._extend_feature(trf, extra_points)\n",
    "        else:\n",
    "            trf = trf.reshape(1, -1)\n",
    "        return trf\n",
    "\n",
    "    def _running_mean(self, x, n):\n",
    "        if n % 2 == 0:\n",
    "            raise ValueError(\"Filter length is not odd\")\n",
    "        aver = np.convolve(x, np.ones((n,)) / n, mode='same')\n",
    "        n_half = int((n - 1) / 2)\n",
    "        corr_length = np.array(range(n_half + 1, n)) / n\n",
    "        aver[:n_half] = np.divide(aver[:n_half], corr_length)\n",
    "        aver[-n_half:] = np.divide(aver[-n_half:], corr_length[::-1])\n",
    "        return aver\n",
    "\n",
    "    def _filter_and_normalize(self, x, filts_len):\n",
    "        for filt_len in filts_len:\n",
    "            x = self._running_mean(x, filt_len)        \n",
    "        x -= np.min(x)\n",
    "        x = x / np.max(x)\n",
    "        return x\n",
    "\n",
    "    def _hfp_feature(self, stft, fs, extra_points, rm_filts_len):\n",
    "        freq_limit = 6000  # Lower limit for high-freq range\n",
    "        freq = np.linspace(0, fs / 2, stft.shape[0])\n",
    "        freq_limit_ind = int(np.argwhere(freq >= freq_limit)[0])\n",
    "        hf_en = np.sum(np.power(stft[freq_limit_ind:, :], 2), axis=0)\n",
    "        hf_en = self._filter_and_normalize(hf_en, rm_filts_len)\n",
    "        if extra_points > 0:\n",
    "            hf_en = self._extend_feature(hf_en, extra_points)\n",
    "        else:\n",
    "            hf_en = hf_en.reshape(1, -1)\n",
    "        return hf_en\n",
    "    \n",
    "    def create_labels(self, audio_filename, time, time_dist_threshold):\n",
    "        file_name, file_extension = os.path.splitext(audio_filename)\n",
    "        with open(file_name + '.txt') as f:\n",
    "            minima = f.readlines()\n",
    "        minima_positions = np.array([float(x.strip()) for x in minima])\n",
    "        minima_no = minima_positions.size\n",
    "        vehicle_count = 0\n",
    "        if minima_positions[0] >= 0:\n",
    "            vehicle_count = minima_no\n",
    "        \n",
    "        labels_separate = np.empty((0, time.size))\n",
    "        for k in range(minima_no):\n",
    "            label_vehicle = time_dist_threshold * np.ones_like(time)\n",
    "            if minima_positions[k] >= 0:\n",
    "                abs_fun = np.abs(time - minima_positions[k])\n",
    "                label_vehicle = np.fmin(label_vehicle, abs_fun)\n",
    "            labels_separate = np.append(labels_separate, label_vehicle.reshape(1, -1), axis=0)\n",
    "    \n",
    "        labels = np.min(labels_separate, axis=0)\n",
    "\n",
    "        return labels, vehicle_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocess()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction and label parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_len = 4096   # Window length in samples\n",
    "hop_perc = 40 # Hop length relative to window size (percents)\n",
    "hop_len = int((hop_perc / 100.0) * window_len) # Hop length in the STFT calculation\n",
    "time_samples_num = 539 # Number of time samples where features are calcuated\n",
    "mel_bands_num = 64 # Number of Mel-bands to calculate Mel-band energy feature\n",
    "signal_len = 20.0 # Signal length in seconds\n",
    "adj_feat_num = 10 # Number of adjacent features to concatenate (only for 1-D features)\n",
    "feat_num = mel_bands_num + 3 * (2 * adj_feat_num + 1) # Number of features (mel_bands_num + 1-D time features)\n",
    "run_mean_filts_len = (11, 5) # Lengths of running mean filters for postprocessing of 1-D features\n",
    "time_dist_threshold = 0.75 # Time distance threshold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " audio_folder = \"../VC-PRG-1_5/\"\n",
    " output_dataset_name = 'dataset_cross-validation.h5'\n",
    "\n",
    "audio_files = [os.path.basename(file) for file in  glob.glob( audio_folder + '*.wav')]\n",
    "audio_files.sort(reverse=False)\n",
    "\n",
    "files_num = len(audio_files)\n",
    "features = np.zeros((files_num, time_samples_num, feat_num))\n",
    "labels = np.zeros((files_num, time_samples_num))\n",
    "places = np.zeros((files_num, ))\n",
    "veh_count = np.zeros((files_num, ))\n",
    "\n",
    "for file_index in tqdm(range(files_num)):\n",
    "    audio_file_name = audio_folder + audio_files[file_index]\n",
    "    y, fs = preprocessor.load_audio(audio_file_name, signal_length=signal_len)\n",
    "\n",
    "    features[file_index, :, :], time = preprocessor.extract_feature(y, fs, window_len, hop_len, mel_bands_num, adj_feat_num, run_mean_filts_len)\n",
    "    labels[file_index, :], veh_count[file_index] = preprocessor.create_labels(audio_file_name, time, time_dist_threshold)\n",
    "    places[file_index] = int(audio_files[file_index][6:8])\n",
    "\n",
    "output_folder = 'datasets/'\n",
    "hf = h5py.File(output_folder + output_dataset_name, 'w')\n",
    "hf.create_dataset('features', data=features, compression=\"gzip\")\n",
    "hf.create_dataset('labels', data=labels, compression=\"gzip\")\n",
    "hf.create_dataset('places', data=places, compression=\"gzip\")\n",
    "hf.create_dataset('vehicle_count', data=veh_count, compression=\"gzip\")\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    " def create_dataset(audio_folder, output_dataset_name, output_folder):\n",
    "    audio_files = [os.path.basename(file) for file in  glob.glob( audio_folder + '*.wav')]\n",
    "    audio_files.sort(reverse=False)\n",
    "\n",
    "    files_num = len(audio_files)\n",
    "    features = np.zeros((files_num, time_samples_num, feat_num))\n",
    "    labels = np.zeros((files_num, time_samples_num))\n",
    "    places = np.zeros((files_num, ))\n",
    "    veh_count = np.zeros((files_num, ))\n",
    "\n",
    "    for file_index in tqdm(range(files_num)):\n",
    "        audio_file_name = audio_folder + audio_files[file_index]\n",
    "        y, fs = preprocessor.load_audio(audio_file_name, signal_length=signal_len)\n",
    "\n",
    "        features[file_index, :, :], time = preprocessor.extract_feature(y, fs, window_len, hop_len, mel_bands_num, adj_feat_num, run_mean_filts_len)\n",
    "        labels[file_index, :], veh_count[file_index] = preprocessor.create_labels(audio_file_name, time, time_dist_threshold)\n",
    "        places[file_index] = int(audio_files[file_index][6:8])\n",
    "\n",
    "    hf = h5py.File(output_folder + output_dataset_name, 'w')\n",
    "    hf.create_dataset('features', data=features, compression=\"gzip\")\n",
    "    hf.create_dataset('labels', data=labels, compression=\"gzip\")\n",
    "    hf.create_dataset('places', data=places, compression=\"gzip\")\n",
    "    hf.create_dataset('vehicle_count', data=veh_count, compression=\"gzip\")\n",
    "    hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_folder = \"../VC-PRG-1_5/\"\n",
    "output_dataset_name = 'dataset_cross-validation.h5'\n",
    "output_folder = 'datasets/'\n",
    "create_dataset(audio_folder, output_dataset_name, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 172/172 [07:15<00:00,  2.53s/it]\n"
     ]
    }
   ],
   "source": [
    "audio_folder = \"../VC-PRG-6/\"\n",
    "output_dataset_name = 'dataset_test_unseen.h5'\n",
    "output_folder = 'datasets/'\n",
    "create_dataset(audio_folder, output_dataset_name, output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43926946470bb827d305256f979d20d06136168b26887207f85f0d5c88e9b0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
