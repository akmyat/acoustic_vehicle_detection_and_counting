{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "import torch\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "from transformers import ASTFeatureExtractor()\n",
    "from transformers import AutoConfig, AutoModelForAudioClassification, TrainingArguments, Trainer\n",
    "\n",
    "from datasets import Audio, Dataset, DatasetDict, ClassLabel, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and Unzip VC-PRG-1_5.zip\n",
    "!wget http://cmp.felk.cvut.cz/data/audio_vc/audio/VC-PRG-1_5.zip\n",
    "!unzip VC-PRG-1_5.zip\n",
    "!rm VC-PRG-1_5.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and Unzip VC-PRG-6.zip\n",
    "!wget http://cmp.felk.cvut.cz/data/audio_vc/audio/VC-PRG-6.zip\n",
    "!unzip VC-PRG-6.zip\n",
    "!rm VC-PRG-6.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/penguin/anaconda3/envs/PT/lib/python3.10/site-packages/transformers/configuration_utils.py:369: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "feature_extractor = ASTFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9', 10: '10', 11: '11', 12: '12', 13: '13', 14: '14'}\n",
      "{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '10': 10, '11': 11, '12': 12, '13': 13, '14': 14}\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 15\n",
    "label2id = {i: str(i) for i in range(NUM_CLASSES)}\n",
    "id2label = {str(i): i for i in range(NUM_CLASSES)}\n",
    "\n",
    "print(label2id)\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../VC-PRG-1_5/\"\n",
    "train_X = sorted(glob.glob(train_path + \"*.wav\"))\n",
    "label_files = sorted(glob.glob(train_path + \"*.txt\"))\n",
    "\n",
    "train_y = []\n",
    "for file in label_files:\n",
    "    with open(file, 'r') as f:\n",
    "        content = f.readlines()\n",
    "        if '-1' in content:\n",
    "            train_y.append(0)\n",
    "        else:\n",
    "            train_y.append(len(content))\n",
    "\n",
    "test_path = \"../VC-PRG-6/\"\n",
    "test_X = sorted(glob.glob(test_path + \"*.wav\"))\n",
    "label_files = sorted(glob.glob(test_path + \"*.txt\"))\n",
    "\n",
    "test_y = []\n",
    "for file in label_files:\n",
    "    with open(file, 'r') as f:\n",
    "        content = f.readlines()\n",
    "        if '-1' in content:\n",
    "            test_y.append(0)\n",
    "        else:\n",
    "            test_y.append(len(content))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file', 'audio', 'label'],\n",
       "        num_rows: 250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['file', 'audio', 'label'],\n",
       "        num_rows: 172\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_audio = Dataset.from_dict(\n",
    "{\n",
    "    'file': [os.path.basename(item) for item in train_X],\n",
    "    'audio': train_X,\n",
    "    'label': train_y,\n",
    "}\n",
    ").cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "test_audio = Dataset.from_dict(\n",
    "{\n",
    "    'file': [os.path.basename(item) for item in test_X],\n",
    "    'audio': test_X,\n",
    "    'label': test_y,\n",
    "}\n",
    ").cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "dataset = DatasetDict()\n",
    "dataset[\"train\"] = train_audio\n",
    "dataset[\"test\"] = test_audio\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays,\n",
    "        sampling_rate=16000,\n",
    "        max_length=16000 * 20,\n",
    "        truncation=True,\n",
    "        )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf6840f04cd465e8441f2977091c9a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/penguin/anaconda3/envs/PT/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  tensor = as_tensor(value)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d342da1e4c478da986b2af0a142a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_values'],\n",
       "        num_rows: 250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_values'],\n",
       "        num_rows: 172\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_function, remove_columns=[\"audio\", \"file\"], batched=True)\n",
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSequenceClassification: ['project_hid.weight', 'project_q.weight', 'project_hid.bias', 'quantizer.weight_proj.bias', 'project_q.bias', 'quantizer.codevectors', 'quantizer.weight_proj.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['projector.weight', 'projector.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(id2label)\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"MIT/ast-finetuned-audioset-10-10-0.4593\",\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    num_labels=num_labels,\n",
    ")\n",
    "\n",
    "model = AutoModelForAudioClassification.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/penguin/anaconda3/envs/PT/lib/python3.10/site-packages/torch/cuda/__init__.py:98: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1672906220333/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "model_name = \"ast\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-ks\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=5,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"test\"],\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/penguin/anaconda3/envs/PT/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 250\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 10\n",
      "  Number of trainable parameters = 94572431\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a38c9162b045759076b17c3a539427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43926946470bb827d305256f979d20d06136168b26887207f85f0d5c88e9b0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
