{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchaudio\n",
    "from torchaudio.transforms import Resample, MelSpectrogram\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib  inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_AUDIO_FOLDER = \"../VC-PRG-1_5/\"\n",
    "TEST_AUDIO_FOLDER = \"../VC-PRG-6/\"\n",
    "\n",
    "SAMPLE_RATE = 22050\n",
    "NUM_SAMPLES = 22050\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = TRAIN_AUDIO_FOLDER\n",
    "\n",
    "class VehicleDataset(Dataset):\n",
    "    def __init__(self, folder_path, device, target_sample_rate, num_samples):\n",
    "        self.audio_files = sorted(glob.glob(path + \"*.wav\"))\n",
    "        self.label_files = sorted(glob.glob(path + \"*.txt\"))\n",
    "        self.device = device\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        label = self.__get_label(item)\n",
    "        waveform, sample_rate = self.__load_audio(item)\n",
    "        waveform = waveform.to(self.device)\n",
    "        waveform = self.__resample_if_necessary(waveform, sample_rate)\n",
    "        waveform = self.__mix_down_if_necessary(waveform)\n",
    "        waveform = self.__cut_if_necessary(waveform)\n",
    "        waveform = self.__right_pad_if_necessary(waveform)\n",
    "\n",
    "        mel_spectrogram = self.__get_mel_spectrogram(waveform)\n",
    "        mel_spectrogram = self.__resize_mel_spectrogram(mel_spectrogram)\n",
    "        mel_spectrogram = self.__convert_to_rgb(mel_spectrogram)\n",
    "        \n",
    "        return mel_spectrogram, label\n",
    "\n",
    "    def __get_label(self, item):\n",
    "        label = 0\n",
    "        with open(self.label_files[item], 'r') as f:\n",
    "            label = len(f.readlines())\n",
    "        return label\n",
    "    \n",
    "    def __load_audio(self, item):\n",
    "        waveform, sample_rate = torchaudio.load(self.audio_files[item])\n",
    "        return waveform, sample_rate\n",
    "    \n",
    "    def __resample_if_necessary(self, waveform, sample_rate):\n",
    "        if sample_rate != self.target_sample_rate:\n",
    "            resampler = Resample(sample_rate, self.target_sample_rate).to(self.device)\n",
    "            waveform = resampler(waveform)\n",
    "        return waveform\n",
    "\n",
    "    def __mix_down_if_necessary(self, waveform):\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        return waveform\n",
    "    \n",
    "    def __cut_if_necessary(self, waveform):\n",
    "        if waveform.shape[1] > self.num_samples:\n",
    "            waveform = waveform[:, :self.num_samples]\n",
    "        return waveform\n",
    "    \n",
    "    def __right_pad_if_necessary(self, waveform):\n",
    "        if waveform.shape[1] < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - waveform.shape[1]\n",
    "            waveform = F.pad(waveform, (0, num_missing_samples))\n",
    "        return waveform\n",
    "    \n",
    "    def __get_mel_spectrogram(self, waveform):\n",
    "        mel_spec_transformer = MelSpectrogram(\n",
    "            sample_rate=self.target_sample_rate,\n",
    "            n_fft=1024,\n",
    "            win_length=None,\n",
    "            hop_length=512,\n",
    "            n_mels=64\n",
    "        ).to(self.device)\n",
    "        mel_spec = mel_spec_transformer(waveform)\n",
    "        return mel_spec\n",
    "\n",
    "    def __resize_mel_spectrogram(self, mel_spec):\n",
    "        mel_spec = Resize((224, 224), interpolation=T.InterpolationMode.BILINEAR)(mel_spec)\n",
    "        return mel_spec\n",
    "\n",
    "    def __convert_to_rgb(self, mel_spec):\n",
    "        mel_spec = T.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x)(mel_spec)\n",
    "        return mel_spec\n",
    "    def __normalize(self, mel_spec):\n",
    "        mel_spec = T.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD)(mel_spec)\n",
    "        return mel_spec\n",
    "\n",
    "vcd  = VehicleDataset(path, device, SAMPLE_RATE, NUM_SAMPLES)\n",
    "\n",
    "print(\"Feature shape: \", vcd[0][0].shape)\n",
    "print(\"Label: \", vcd[0][1])\n",
    "\n",
    "print(\"Feature shape: \", vcd[1][0].shape)\n",
    "print(\"Label: \", vcd[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = vcd\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_loss = list()\n",
    "train_acc = list()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        features, labels = data\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        labels = nn.functional.one_hot(labels, num_classes=10).float()\n",
    "\n",
    "        predictions = model(features)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        total += labels.size(0)\n",
    "        correct += (torch.argmax(predictions, 1) == torch.argmax(labels, 1)).sum().item()\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch [{epoch + 1} / {NUM_EPOCHS}] loss: {running_loss / 10:.3f}\")\n",
    "    \n",
    "    train_loss.append(running_loss / len(train_loader))\n",
    "    train_acc.append(correct / total * 100.0)\n",
    "\n",
    "torch.save(model.state_dict(), \"vcd_DeiT_model.pth\")\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax1.plot(train_acc, '-o')\n",
    "ax1.set_title = 'Train Accuracy'\n",
    "ax1.set_xlabel = 'Epoch'\n",
    "ax1.set_ylabel = 'Accuracy'\n",
    "\n",
    "ax2.plot(train_loss, '-o')\n",
    "ax2.set_title = 'Train Loss'\n",
    "ax2.set_xlabel = 'Epoch'\n",
    "ax2.set_ylabel = 'Loss'\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = TEST_AUDIO_FOLDER\n",
    "test_data  = VehicleDataset(path, device, SAMPLE_RATE, NUM_SAMPLES) \n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        features, labels = data\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        \n",
    "        predictions = model(features)\n",
    "        _, predictions = torch.max(predictions, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "print(f'Accuracy: {100 * correct // total} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43926946470bb827d305256f979d20d06136168b26887207f85f0d5c88e9b0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
